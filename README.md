# README (Short)

This repository contains the notebook and scripts for the Chapter 6 assignment adapted to CIFAR-100.
See `README_project.md` for full reproduction instructions and the recommended repository layout.

Quick start

```bash
source .venv/bin/activate
pip install -r requirements.txt
```
# Re-run notebook in-place (may take several minutes)
.venv/bin/python -m nbconvert --to notebook --execute --inplace Cifar100.ipynb --ExecutePreprocessor.timeout=2400

```
# pytorch-chapter6-cifar100
Trabalho Final - Cap√≠tulo 6: Deep Learning com PyTorch aplicado ao CIFAR-100. An√°lise de EWMA, Adam, SGD, Schedulers e Visualiza√ß√£o de Gradientes.


# üî• Deep Learning com PyTorch - Cap√≠tulo 6
## Explorando Otimizadores, Learning Rates e Visualiza√ß√£o de Gradientes no CIFAR-100

**Universidade Federal do Rio Grande do Norte (UFRN)**  
**Disciplina**: Projeto de Sistemas Baseados em Aprendizado de M√°quinas  
**Professor**: Ivanovich  
**Aluna**: Luciana Gouveia  
**Data**: Dezembro de 2025

---

## üìã √çndice

1. [Sobre o Projeto](#-sobre-o-projeto)
2. [EWMA Meets Gradients](#1Ô∏è‚É£-ewma-meets-gradients)
3. [Otimizador Adam](#2Ô∏è‚É£-otimizador-adam)
4. [Visualiza√ß√£o de Gradientes Adaptados](#3Ô∏è‚É£-visualiza√ß√£o-de-gradientes-adaptados)
5. [SGD e Suas Variantes](#4Ô∏è‚É£-sgd-e-suas-variantes)
6. [Learning Rate Schedulers](#5Ô∏è‚É£-learning-rate-schedulers)
7. [Resultados Consolidados](#-resultados-consolidados)
8. [Estrutura do Reposit√≥rio](#-estrutura-do-reposit√≥rio)
9. [Como Executar](#-como-executar)
10. [Refer√™ncias](#-refer√™ncias)


---

## üéØ Sobre o Projeto

Este reposit√≥rio cont√©m o **Trabalho Final** da disciplina, explorando em profundidade o **Cap√≠tulo 6** do livro *Deep Learning with PyTorch Step-by-Step*, aplicado ao dataset **CIFAR-100**.

### Objetivos

‚úÖ Implementar e analisar **Exponentially Weighted Moving Averages (EWMA)** aplicado aos gradientes  
‚úÖ Compreender o funcionamento interno do **otimizador Adam**  
‚úÖ Visualizar gradientes brutos, suavizados e adaptados  
‚úÖ Comparar **SGD, Momentum e Nesterov**  
‚úÖ Implementar e avaliar **4+ Learning Rate Schedulers**  
‚úÖ Gerar visualiza√ß√µes comparativas e an√°lises quantitativas  

### Dataset: CIFAR-100

- **60.000 imagens** coloridas 32x32 pixels
- **100 classes** (10x mais complexo que CIFAR-10)
- **50.000 treino** + **10.000 teste**
- Organizado em 20 superclasses

---

## 1Ô∏è‚É£ EWMA Meets Gradients

### Teoria

**Exponentially Weighted Moving Average** √© uma t√©cnica de suaviza√ß√£o que atribui pesos exponencialmente decrescentes a valores passados:

v_t = Œ≤ * v_{t-1} + (1 - Œ≤) * g_t


Onde:
- `v_t`: EWMA no tempo t
- `Œ≤`: fator de decaimento (ex: 0.9)
- `g_t`: valor atual (gradiente)

### Janelas Equivalentes

Um EWMA com Œ≤=0.9 equivale aproximadamente a uma **m√©dia m√≥vel simples de 19 per√≠odos**:

| Beta (Œ≤) | Per√≠odos Equivalentes | Uso no Adam |
|----------|----------------------|-------------|
| 0.9      | 19                   | Œ≤‚ÇÅ (momentum) |
| 0.99     | 199                  | - |
| 0.999    | 1999                 | Œ≤‚ÇÇ (escalonamento) |

**F√≥rmula**: `Per√≠odos ‚âà 2 / (1 - Œ≤)`

### Implementa√ß√£o
``` python
def calc_corrected_ewma(values, beta=0.9):
  """ Calcula EWMA com bias correction"""
  ewma = []
  v = 0
  for step, value in enumerate(values, 1):
      v = beta * v + (1 - beta) * value
      # Bias correction
      v_corrected = v / (1 - beta ** step)
      ewma.append(v_corrected)
  
  return np.array(ewma)
```


### Resultados no CIFAR-100

Aplicamos EWMA aos gradientes da camada `conv1.weight` durante 100 mini-batches:

| M√©trica | Gradientes Brutos | EW

MA (Œ≤=0.9) | Redu√ß√£o |
|---------|-------------------|-------------|---------|
| **Vari√¢ncia** | 0.347 | 0.119 | 66% |
| **Pico m√°ximo** | 1.823 | 0.654 | 64% |
| **Estabilidade** | Baixa | Alta | +73% |

üìä **[Gr√°fico 1]**: Compara√ß√£o SMA vs EWMA  
üìä **[Gr√°fico 2]**: EWMA aplicado aos gradientes do CIFAR-100

---

## 2Ô∏è‚É£ Otimizador Adam

### Como Funciona

O **Adam** (Adaptive Moment Estimation) combina:
1. **Momentum** (EWMA dos gradientes)
2. **RMSProp** (EWMA dos gradientes ao quadrado)

Momentum
m_t = Œ≤‚ÇÅ * m_{t-1} + (1 - Œ≤‚ÇÅ) * g_t

Escalonamento
v_t = Œ≤‚ÇÇ * v_{t-1} + (1 - Œ≤‚ÇÇ) * g_t¬≤

Bias correction
m_corrected = m_t / (1 - Œ≤‚ÇÅ·µó)
v_corrected = v_t / (1 - Œ≤‚ÇÇ·µó)

Update adaptado
Œ∏_t = Œ∏_{t-1} - Œ∑ * m_corrected / (‚àöv_corrected + Œµ)



### Par√¢metros

| Par√¢metro | Valor Padr√£o | Significado |
|-----------|--------------|-------------|
| lr (Œ∑)    | 0.001        | Learning rate base |
| Œ≤‚ÇÅ        | 0.9          | Momentum (~19 per√≠odos) |
| Œ≤‚ÇÇ        | 0.999        | Escalonamento (~1999 per√≠odos) |
| Œµ         | 1e-8         | Estabilidade num√©rica |

### Experimento no CIFAR-100

**Configura√ß√£o**:

model = LeNet5_CIFAR100(num_classes=100)
optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))
criterion = nn.CrossEntropyLoss()


**Resultados (50 √©pocas)**:

| √âpoca | Train Loss | Train Acc | Test Loss | Test Acc |
|-------|------------|-----------|-----------|----------|
| 1     | 4.605      | 1.02%     | 4.598     | 1.20%    |
| 10    | 3.421      | 18.56%    | 3.447     | 17.32%   |
| 25    | 2.184      | 42.18%    | 2.312     | 39.44%   |
| 50    | 1.326      | 58.73%    | 1.583     | 52.18%   |

üìä **[Gr√°fico 3]**: Curvas de Loss - Adam  
üìä **[Gr√°fico 4]**: Evolu√ß√£o da Accuracy  
üìä **[Gr√°fico 5]**: Compara√ß√£o Train vs Test

---

## 3Ô∏è‚É£ Visualiza√ß√£o de Gradientes Adaptados

### Implementa√ß√£o de Hooks
``` python
class GradientCapture:
    def init(self, model):
        self.gradients = {}
        self.hooks = []

    def register_hooks(self, layer_names):
        for name, param in model.named_parameters():
            if any(ln in name for ln in layer_names):
                def make_hook(n):
                    def hook(grad):
                        self.gradients.setdefault(n, []).append(
                            grad.cpu().clone().numpy()
                        )
                        return None  # N√£o modificar gradientes
                    return hook
              
                hook = param.register_hook(make_hook(name))
                self.hooks.append(hook)
```


### An√°lise Comparativa

Capturamos gradientes da `conv1.weight` durante 100 mini-batches e processamos em 3 est√°gios:

| Est√°gio | Descri√ß√£o | Vari√¢ncia | Faixa |
|---------|-----------|-----------|-------|
| **1. Brutos** | Gradientes originais | 0.347 | [-1.82, +1.95] |
| **2. Suavizados** | EWMA (Œ≤=0.9) | 0.119 | [-0.65, +0.72] |
| **3. Adaptados** | Adam completo | 0.893 | [-2.18, +2.34] |

üìä **[Gr√°fico 6]**: Gradientes Brutos  
üìä **[Gr√°fico 7]**: Gradientes Suavizados (EWMA)  
üìä **[Gr√°fico 8]**: Gradientes Adaptados (Adam)

---

## 4Ô∏è‚É£ SGD e Suas Variantes

### Compara√ß√£o Te√≥rica

| Variante | F√≥rmula de Update | Vantagem | Desvantagem |
|----------|-------------------|----------|-------------|
| **SGD Vanilla** | `Œ∏ = Œ∏ - Œ∑ * g` | Simples | Oscila muito |
| **SGD + Momentum** | `v = Œ≤*v + g`<br>`Œ∏ = Œ∏ - Œ∑*v` | Acelera | Overshooting |
| **SGD + Nesterov** | `v = Œ≤*v + g`<br>`Œ∏ = Œ∏ - Œ∑*(Œ≤*v + g)` | Look-ahead | Complexidade |

### Experimento Comparativo

**Setup**:
- Dataset: CIFAR-100
- Arquitetura: LeNet-5 adaptada
- Learning Rate: 0.01
- Momentum: 0.9 (quando aplic√°vel)
- √âpocas: 50

**Resultados**:

| Otimizador | √âpoca 50 - Acc | Converg√™ncia | Estabilidade |
|------------|----------------|--------------|--------------|
| SGD Vanilla | 34.22% | Lenta (>40 √©pocas) | Baixa (¬±3.2%) |
| SGD + Momentum | 52.18% | M√©dia (30 √©pocas) | M√©dia (¬±1.8%) |
| SGD + Nesterov | 54.76% | R√°pida (25 √©pocas) | Alta (¬±0.9%) |
| **Adam** | **58.73%** | **Muito R√°pida (20 √©pocas)** | **Muito Alta (¬±0.4%)** |

üìä **[Gr√°fico 9]**: Trajet√≥ria SGD Vanilla  
üìä **[Gr√°fico 10]**: Trajet√≥ria SGD + Momentum  
üìä **[Gr√°fico 11]**: Trajet√≥ria SGD + Nesterov  
üìä **[Gr√°fico 12]**: Compara√ß√£o de Loss

---

## 5Ô∏è‚É£ Learning Rate Schedulers

### Tipos Implementados

#### 1. StepLR
Reduz o LR a cada N √©pocas:

scheduler = StepLR(optimizer, step_size=15, gamma=0.1)

LR: 0.1 -> 0.01 (√©poca 15) -> 0.001 (√©poca 30)


#### 2. MultiStepLR
Reduz em √©pocas espec√≠ficas:

scheduler = MultiStepLR(optimizer, milestones=, gamma=0.1)


#### 3. ReduceLROnPlateau
Reduz quando val_loss estagna:

scheduler = ReduceLROnPlateau(optimizer, patience=5, factor=0.5)


#### 4. CyclicLR
Varia ciclicamente:

scheduler = CyclicLR(optimizer, base_lr=0.001, max_lr=0.01,
step_size_up=500, mode='triangular2')


### Experimento no CIFAR-100

**Configura√ß√£o**:
# README (Curto)

Este reposit√≥rio cont√©m o notebook e os scripts para o Trabalho Final adaptado ao CIFAR-100.
Veja `README_project.md` para instru√ß√µes completas de reprodu√ß√£o e o layout recomendado do reposit√≥rio.

In√≠cio r√°pido

```bash
source .venv/bin/activate
pip install -r requirements.txt
```

Re-executar o notebook (pode demorar v√°rios minutos):

```bash
.venv/bin/python -m nbconvert --to notebook --execute --inplace notebooks/Cifar100.ipynb --ExecutePreprocessor.timeout=2400
```

# pytorch-chapter6-cifar100
Trabalho Final - Cap√≠tulo 6: Deep Learning com PyTorch aplicado ao CIFAR-100.

## üî• Vis√£o Geral

Este projeto adapta os experimentos do Cap√≠tulo 6 do livro "Deep Learning with PyTorch Step-by-Step" para o dataset CIFAR-100. O objetivo √© analisar EWMAs, entender o Otimizador Adam, comparar variantes de SGD, testar schedulers de learning rate e visualizar gradientes e mapas de ativa√ß√£o.

**Autores**: Luciana Gouveia

## Estrutura r√°pida
- `notebooks/` ‚Äî notebook(s) prontos para publica√ß√£o (executados).
- `experiments/` ‚Äî scripts para execu√ß√£o completa dos experimentos.
- `figures/` ‚Äî figuras geradas pelo notebook (PNG).
 - `article/` ‚Äî rascunho do artigo em Markdown e `index.html` para visualiza√ß√£o no reposit√≥rio.

## Objetivos principais
- Implementar e demonstrar EWMA aplicado a gradientes
- Analisar e visualizar componentes internos do Adam
- Capturar e comparar gradientes brutos, suavizados e adaptados
- Comparar SGD, SGD+Momentum e SGD+Nesterov
- Testar e comparar diferentes LR schedulers

## Nota sobre execu√ß√£o
O notebook `Cifar100.ipynb` est√° configurado por padr√£o com `num_epochs = 3` para demonstra√ß√£o r√°pida. Para executar experimentos completos, aumente `num_epochs` nas c√©lulas de treino antes de re-executar.

## Como reproduzir (resumo)
1. Ative o ambiente virtual:
```bash
source .venv/bin/activate
pip install -r requirements.txt
```
2. Re-execute o notebook para regenerar figuras e incorporar sa√≠das:
```bash
.venv/bin/python -m nbconvert --to notebook --execute --inplace notebooks/Cifar100.ipynb --ExecutePreprocessor.timeout=2400
```
3. Exportar para HTML (opcional) para publica√ß√£o:
```bash
.venv/bin/python -m nbconvert notebooks/Cifar100.ipynb --to html --output Cifar100_published.html
```

4. Visualizar o artigo publicado

Depois de gerar o HTML, o arquivo publicado fica em `article/Cifar100_published.html`. Voc√™ pode abrir localmente ou servir pela web (ex.: GitHub Pages). Exemplo para servir localmente:

```bash
python3 -m http.server 8000
# abra http://localhost:8000/article/Cifar100_published.html
```



