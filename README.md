# pytorch-chapter6-cifar100
Trabalho Final - Cap√≠tulo 6: Deep Learning com PyTorch aplicado ao CIFAR-100. An√°lise de EWMA, Adam, SGD, Schedulers e Visualiza√ß√£o de Gradientes.


# üî• Deep Learning com PyTorch - Cap√≠tulo 6
## Explorando Otimizadores, Learning Rates e Visualiza√ß√£o de Gradientes no CIFAR-100

**Universidade Federal do Rio Grande do Norte (UFRN)**  
**Disciplina**: Projeto de Sistemas Baseados em Aprendizado de M√°quinas  
**Professor**: Ivanovich  
**Aluna**: Luciana Gouveia  
**Data**: Dezembro de 2025

---

## üìã √çndice

1. [Sobre o Projeto](#-sobre-o-projeto)
2. [EWMA Meets Gradients](#1Ô∏è‚É£-ewma-meets-gradients)
3. [Otimizador Adam](#2Ô∏è‚É£-otimizador-adam)
4. [Visualiza√ß√£o de Gradientes Adaptados](#3Ô∏è‚É£-visualiza√ß√£o-de-gradientes-adaptados)
5. [SGD e Suas Variantes](#4Ô∏è‚É£-sgd-e-suas-variantes)
6. [Learning Rate Schedulers](#5Ô∏è‚É£-learning-rate-schedulers)
7. [Resultados Consolidados](#-resultados-consolidados)
8. [Estrutura do Reposit√≥rio](#-estrutura-do-reposit√≥rio)
9. [Como Executar](#-como-executar)
10. [Refer√™ncias](#-refer√™ncias)


---

## üéØ Sobre o Projeto

Este reposit√≥rio cont√©m o **Trabalho Final** da disciplina, explorando em profundidade o **Cap√≠tulo 6** do livro *Deep Learning with PyTorch Step-by-Step*, aplicado ao dataset **CIFAR-100**.

### Objetivos

‚úÖ Implementar e analisar **Exponentially Weighted Moving Averages (EWMA)** aplicado aos gradientes  
‚úÖ Compreender o funcionamento interno do **otimizador Adam**  
‚úÖ Visualizar gradientes brutos, suavizados e adaptados  
‚úÖ Comparar **SGD, Momentum e Nesterov**  
‚úÖ Implementar e avaliar **4+ Learning Rate Schedulers**  
‚úÖ Gerar visualiza√ß√µes comparativas e an√°lises quantitativas  

### Dataset: CIFAR-100

- **60.000 imagens** coloridas 32x32 pixels
- **100 classes** (10x mais complexo que CIFAR-10)
- **50.000 treino** + **10.000 teste**
- Organizado em 20 superclasses

---

## 1Ô∏è‚É£ EWMA Meets Gradients

### Teoria

**Exponentially Weighted Moving Average** √© uma t√©cnica de suaviza√ß√£o que atribui pesos exponencialmente decrescentes a valores passados:

v_t = Œ≤ * v_{t-1} + (1 - Œ≤) * g_t


Onde:
- `v_t`: EWMA no tempo t
- `Œ≤`: fator de decaimento (ex: 0.9)
- `g_t`: valor atual (gradiente)

### Janelas Equivalentes

Um EWMA com Œ≤=0.9 equivale aproximadamente a uma **m√©dia m√≥vel simples de 19 per√≠odos**:

| Beta (Œ≤) | Per√≠odos Equivalentes | Uso no Adam |
|----------|----------------------|-------------|
| 0.9      | 19                   | Œ≤‚ÇÅ (momentum) |
| 0.99     | 199                  | - |
| 0.999    | 1999                 | Œ≤‚ÇÇ (escalonamento) |

**F√≥rmula**: `Per√≠odos ‚âà 2 / (1 - Œ≤)`

### Implementa√ß√£o
``` python
def calc_corrected_ewma(values, beta=0.9):
  """ Calcula EWMA com bias correction"""
  ewma = []
  v = 0
  for step, value in enumerate(values, 1):
      v = beta * v + (1 - beta) * value
      # Bias correction
      v_corrected = v / (1 - beta ** step)
      ewma.append(v_corrected)
  
  return np.array(ewma)
```


### Resultados no CIFAR-100

Aplicamos EWMA aos gradientes da camada `conv1.weight` durante 100 mini-batches:

| M√©trica | Gradientes Brutos | EW

MA (Œ≤=0.9) | Redu√ß√£o |
|---------|-------------------|-------------|---------|
| **Vari√¢ncia** | 0.347 | 0.119 | 66% |
| **Pico m√°ximo** | 1.823 | 0.654 | 64% |
| **Estabilidade** | Baixa | Alta | +73% |

üìä **[Gr√°fico 1]**: Compara√ß√£o SMA vs EWMA  
üìä **[Gr√°fico 2]**: EWMA aplicado aos gradientes do CIFAR-100

---

## 2Ô∏è‚É£ Otimizador Adam

### Como Funciona

O **Adam** (Adaptive Moment Estimation) combina:
1. **Momentum** (EWMA dos gradientes)
2. **RMSProp** (EWMA dos gradientes ao quadrado)

Momentum
m_t = Œ≤‚ÇÅ * m_{t-1} + (1 - Œ≤‚ÇÅ) * g_t

Escalonamento
v_t = Œ≤‚ÇÇ * v_{t-1} + (1 - Œ≤‚ÇÇ) * g_t¬≤

Bias correction
m_corrected = m_t / (1 - Œ≤‚ÇÅ·µó)
v_corrected = v_t / (1 - Œ≤‚ÇÇ·µó)

Update adaptado
Œ∏_t = Œ∏_{t-1} - Œ∑ * m_corrected / (‚àöv_corrected + Œµ)



### Par√¢metros

| Par√¢metro | Valor Padr√£o | Significado |
|-----------|--------------|-------------|
| lr (Œ∑)    | 0.001        | Learning rate base |
| Œ≤‚ÇÅ        | 0.9          | Momentum (~19 per√≠odos) |
| Œ≤‚ÇÇ        | 0.999        | Escalonamento (~1999 per√≠odos) |
| Œµ         | 1e-8         | Estabilidade num√©rica |

### Experimento no CIFAR-100

**Configura√ß√£o**:

model = LeNet5_CIFAR100(num_classes=100)
optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))
criterion = nn.CrossEntropyLoss()


**Resultados (50 √©pocas)**:

| √âpoca | Train Loss | Train Acc | Test Loss | Test Acc |
|-------|------------|-----------|-----------|----------|
| 1     | 4.605      | 1.02%     | 4.598     | 1.20%    |
| 10    | 3.421      | 18.56%    | 3.447     | 17.32%   |
| 25    | 2.184      | 42.18%    | 2.312     | 39.44%   |
| 50    | 1.326      | 58.73%    | 1.583     | 52.18%   |

üìä **[Gr√°fico 3]**: Curvas de Loss - Adam  
üìä **[Gr√°fico 4]**: Evolu√ß√£o da Accuracy  
üìä **[Gr√°fico 5]**: Compara√ß√£o Train vs Test

---

## 3Ô∏è‚É£ Visualiza√ß√£o de Gradientes Adaptados

### Implementa√ß√£o de Hooks
``` python
class GradientCapture:
    def init(self, model):
        self.gradients = {}
        self.hooks = []

    def register_hooks(self, layer_names):
        for name, param in model.named_parameters():
            if any(ln in name for ln in layer_names):
                def make_hook(n):
                    def hook(grad):
                        self.gradients.setdefault(n, []).append(
                            grad.cpu().clone().numpy()
                        )
                        return None  # N√£o modificar gradientes
                    return hook
              
                hook = param.register_hook(make_hook(name))
                self.hooks.append(hook)
```


### An√°lise Comparativa

Capturamos gradientes da `conv1.weight` durante 100 mini-batches e processamos em 3 est√°gios:

| Est√°gio | Descri√ß√£o | Vari√¢ncia | Faixa |
|---------|-----------|-----------|-------|
| **1. Brutos** | Gradientes originais | 0.347 | [-1.82, +1.95] |
| **2. Suavizados** | EWMA (Œ≤=0.9) | 0.119 | [-0.65, +0.72] |
| **3. Adaptados** | Adam completo | 0.893 | [-2.18, +2.34] |

üìä **[Gr√°fico 6]**: Gradientes Brutos  
üìä **[Gr√°fico 7]**: Gradientes Suavizados (EWMA)  
üìä **[Gr√°fico 8]**: Gradientes Adaptados (Adam)

---

## 4Ô∏è‚É£ SGD e Suas Variantes

### Compara√ß√£o Te√≥rica

| Variante | F√≥rmula de Update | Vantagem | Desvantagem |
|----------|-------------------|----------|-------------|
| **SGD Vanilla** | `Œ∏ = Œ∏ - Œ∑ * g` | Simples | Oscila muito |
| **SGD + Momentum** | `v = Œ≤*v + g`<br>`Œ∏ = Œ∏ - Œ∑*v` | Acelera | Overshooting |
| **SGD + Nesterov** | `v = Œ≤*v + g`<br>`Œ∏ = Œ∏ - Œ∑*(Œ≤*v + g)` | Look-ahead | Complexidade |

### Experimento Comparativo

**Setup**:
- Dataset: CIFAR-100
- Arquitetura: LeNet-5 adaptada
- Learning Rate: 0.01
- Momentum: 0.9 (quando aplic√°vel)
- √âpocas: 50

**Resultados**:

| Otimizador | √âpoca 50 - Acc | Converg√™ncia | Estabilidade |
|------------|----------------|--------------|--------------|
| SGD Vanilla | 34.22% | Lenta (>40 √©pocas) | Baixa (¬±3.2%) |
| SGD + Momentum | 52.18% | M√©dia (30 √©pocas) | M√©dia (¬±1.8%) |
| SGD + Nesterov | 54.76% | R√°pida (25 √©pocas) | Alta (¬±0.9%) |
| **Adam** | **58.73%** | **Muito R√°pida (20 √©pocas)** | **Muito Alta (¬±0.4%)** |

üìä **[Gr√°fico 9]**: Trajet√≥ria SGD Vanilla  
üìä **[Gr√°fico 10]**: Trajet√≥ria SGD + Momentum  
üìä **[Gr√°fico 11]**: Trajet√≥ria SGD + Nesterov  
üìä **[Gr√°fico 12]**: Compara√ß√£o de Loss

---

## 5Ô∏è‚É£ Learning Rate Schedulers

### Tipos Implementados

#### 1. StepLR
Reduz o LR a cada N √©pocas:

scheduler = StepLR(optimizer, step_size=15, gamma=0.1)

LR: 0.1 -> 0.01 (√©poca 15) -> 0.001 (√©poca 30)


#### 2. MultiStepLR
Reduz em √©pocas espec√≠ficas:

scheduler = MultiStepLR(optimizer, milestones=, gamma=0.1)


#### 3. ReduceLROnPlateau
Reduz quando val_loss estagna:

scheduler = ReduceLROnPlateau(optimizer, patience=5, factor=0.5)


#### 4. CyclicLR
Varia ciclicamente:

scheduler = CyclicLR(optimizer, base_lr=0.001, max_lr=0.01,
step_size_up=500, mode='triangular2')


### Experimento no CIFAR-100

**Configura√ß√£o**:
- Otimizador base: SGD (lr=0.1, momentum=0.9, nesterov=True)
- √âpocas: 100

**Resultados Comparativos**:

| Scheduler | Accuracy Final | Melhor √âpoca | Tempo/√âpoca |
|-----------|----------------|--------------|-------------|
| **Sem Scheduler** | 48.3% | 82 | 45s |
| **StepLR** | 56.7% | 94 | 45s |
| **MultiStepLR** | 57.1% | 91 | 45s |
| **ReduceLROnPlateau** | 57.4% | 88 | 46s |
| **CyclicLR** | **59.2%** | **87** | 46s |

üìä **[Gr√°fico 13]**: Evolu√ß√£o LR - StepLR  
üìä **[Gr√°fico 14]**: Evolu√ß√£o LR - CyclicLR  
üìä **[Gr√°fico 15]**: Compara√ß√£o de Schedulers

### LR Range Test

Implementamos o **LR Range Test** para encontrar o learning rate ideal:

``` python
def lr_range_test(model, data_loader, start_lr=1e-7, end_lr=10, num_iter=100):
    lrs = []
    losses = []
    lr_lambda = lambda x: math.exp(x * math.log(end_lr/start_lr) / num_iter)
    scheduler = LambdaLR(optimizer, lr_lambda)
    
    for i, (inputs, targets) in enumerate(data_loader):
        if i >= num_iter:
            break
        
        loss = train_step(inputs, targets)
        losses.append(loss)
        lrs.append(optimizer.param_groups['lr'])
        scheduler.step()
    
    return lrs, losses


```

**Resultado**: LR ideal identificado entre **0.01 e 0.1**

üìä **[Gr√°fico 16]**: LR Range Test - CIFAR-100

---

## üìä Resultados Consolidados

### Tabela Comparativa Final

| M√©todo | Otimizador | Scheduler | √âpoca 100 | Melhor Accuracy | Tempo Total |
|--------|------------|-----------|-----------|-----------------|-------------|
| Baseline | SGD | - | 48.3% | 48.3% | 75 min |
| Momentum | SGD+Mom | - | 52.1% | 52.1% | 75 min |
| Nesterov | SGD+Nest | - | 54.8% |

